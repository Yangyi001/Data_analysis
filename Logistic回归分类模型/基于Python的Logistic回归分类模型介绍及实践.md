## 基于Python的Logistic回归分类模型介绍及实践

 - [ ] *这是一篇学习的总结笔记*
 - [ ] 参考自《从零开始学数据分析与挖掘》 [中]刘顺祥 著 
 - [ ] *完整代码及实践所用数据集等资料放置于：[Github](https://github.com/Yangyi001/Data_analysis/tree/master/Kmeans%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90)*

在实际的数据挖掘中，站在预测类问题的角度来看，对于连续型变量的预测，可以使用多元线性回归模型、岭回归模型或者LASSO回归模型来解决；而对于离散型变量的判别，例如以下情况：根据肿瘤病人的各项数据，判断其是否为恶性肿瘤、根据用户的消费行为与信用分判断是否为优质客户等，对于这类结果为离散型的问题（是 or 否）又该如何解决呢？    

下面介绍的 Logistic 回归模型就专门被用来解决二分类的离散问题，它与线性回归模型有着千丝万缕的关系，但与之相比，它属于非线性模型。   

*二分类问题：如上面所举的例子：肿瘤是否为恶行、客户是否为优质客户都属于二分问题。*    

Logistic 回归模型目前是最受工业界所青睐的模型之一，该模型的一个最大特色，就是相对于其他很多分类算法（SVM、神经网络、随机森林等）来说，具有很强的可解释性。接下来我们通过介绍了解以下内容：

- 如何构建 Logistic 回归模型以及参数求解
- Logistic 回归模型的参数解释
- 模型效果评估的常用方法
- 基于该模型完成一个小项目

### Logistic 模型的构建

前面所说，Logistic回归是一种非线性的回归模型，但它又和线性回归模型有关系，所以属于广义的线性回归分析模型。   

我们可以借助该模型实现两大用途：
1. 寻找“危险”因素，例如：医学界常用模型中的优势比寻找影响某种疾病的“坏”因素。
2. 判别样本所属的类别，例如根据手机设备的记录数据判断用户是处于行走状态还是跑步状态。   

如果我们直接使用线性回归模型对离散型的因变量进行建模，很容易会导致错误的结果（我们知道，线性回归的因变量是连续型的，无法很好地确定划分类别的阈值）。而如果我们对线性回归模型做某种变换，能够使得预测值被“压缩”在 0~1 之间，那么这个范围就可以理解为某个二分类问题的结果发生与否的概率。     

此变换函数应该具有如下性质：
- 线性回归的预测值越大，转换后的概率值就越接近于 1 ，反之

Logistic 回归模型就是通过对线性回归模型做 Logit 变换得到的，变换函数如下：$$g(z) = \frac{1}{1+e^{-z}}$$![](.\charts\Logit函数的可视化.png)
其中 $z\in(-\infty, +\infty)$ 。很明显，当 $z$ 趋近于无穷大时，$g(z)$逼近于 1 ；反之，当 $z$ 趋于负无穷大时，$g(z)$逼近于 0 。     

假定线性回归模型为：$$z=\beta_0+\beta_1x_1+\beta_2x_2+...+\beta_px_p$$则Logit变换为：$$g(z)=\frac{1}{1+e^{-(\beta_0+\beta_1x_1+\beta_2x_2+...+\beta_px_p)}}=h_\beta(X)$$上式中的$h_\beta(X)$也被称为 Logistic 回归模型，它将线性回归模型的预测值通过非线性的 Logit 函数转换为 [0, 1] 之间的概率值，之后我们就可以依赖该概率值去评估样本所属的类别。     

**模型变换**    

假定在已知 $X$ 和 $\beta$ 的情况下，因变量取 1 和 0 的条件概率分别用 $h_\beta(X)$ 和 $1-h_\beta(X)$ 表示，则这个条件概率可以表示为：$$P(y=1|X;\beta)=h_\beta(X)=p$$$$P(y=0|X;\beta)=1-h_\beta(X)=1-p$$则两个概率的商（即优势比：优势比又叫发生比，代表了某个事件发生与不发生的概率比值，其范围落在$(0, +\infty)$之间）：$$\frac{p}{1-p}=\frac{h_\beta(X)}{1-h_\beta(X)}\\=(\frac{\frac{1}{1+e^{-(\beta_0+\beta_1x_1+\beta_2x_2+...+\beta_px_p)}}}{1-\frac{1}{1+e^{-(\beta_0+\beta_1x_1+\beta_2x_2+...+\beta_px_p)}}})\\=\frac{1}{e^{-(\beta_0+\beta_1x_1+\beta_2x_2+...+\beta_px_p)}}\\=e^{\beta_0+\beta_1x_1+\beta_2x_2+...+\beta_px_p}$$如果将上述的优势比取对数，则如上公式可以表为：$$log(\frac{p}{1-p})=log(e^{\beta_0+\beta_1x_1+\beta_2x_2+...+\beta_px_p})\\=\beta_0+\beta_1x_1+\beta_2x_2+...+\beta_px_p$$就将 Logistic 回归模型转换为线性回归模型的形式。    
但此处需要注意：因变量不再是实际y的值，而是与概率相关的对数值，所以此处我们不能使用线性回归的方式求解位置参数$\beta$，而是采用极大似然估计法。    

#### Logistic模型的参数求解

我们可以将事件发生与不发生的概率的两个公式重写如下：$$P(y|X;\beta)=h_\beta(X)^y\times(1-h_\beta(X))^{1-y}$$
**极大似然估计**  

与线性回归求解残差最小值不同，此处我们要求解未知参数 $\beta$ ，就需要构建一个目标函数，这个函数就是似然函数。其统计背景是：如果数据中每个样本都是相互独立的，则n个样本发生的联合概率就是各样本事件发生的概率乘积。故似然函数可以表示为：$$L(\beta)=\prod_{i=1}^nP(y^{(i)}|x^{(i)};\beta)\\=\prod_{i=1}^nh_\beta(x^{(i)})^{y^{(i)}}\times(1-h_\beta(x^{(i)}))^{1-y^{(i)}}$$其中，上标i表示第i个样本。接下来要做的就是求解使得目标函数达到最大值的未知参数$\beta$，上面提到的似然估计法就是实现这个目标的方法，有关似然函数的详细介绍及其背后的数学原理请参考[知乎上的这一篇](https://zhuanlan.zhihu.com/p/26614750)

#### Logistic模型的参数解释

前面我们介绍了发生比的概念，即某个事件发生与不发生的概率比值。发生比的作用只能解释为在同一组事件中发生与不发生的倍数，但是使用发生比率，就可以解释参数$\beta$的含义。    

在线性回归模型中，我们对参数$\beta$的解释是固定其他参数，$\beta$系数对应的参数每变化一个单位因变量的变化情况，固定其他参数是为了消除其他参数的影响。而在 Logistic 模型中，使用发生比率也是为了消除其他参数的影响。    

假设是否患肿瘤的因素有性别和肿瘤体积两个变量，通过建立模型我们得到了对应的系数$\beta_1$和$\beta_2$，则 Logistic 回归模型可以按照事件发生比的形式改写为：$$odds=\frac{p}{1-p}=e^{\beta_0+\beta_1Gender+\beta_2Volum}\\=e^{\beta_0}\times e^{\beta_1Gender}\times e^{\beta_2Volum}$$接下来我们分别以性别变量和肿瘤体积变量来解释$\beta_1$和$\beta_2$的含义。   

假设性别中以 1 表示男性，以 0 表示女性，则发生比率：$$\frac{odds_1}{odds_0}=\frac{e^{\beta_0}\times e^{\beta_1\times 1}\times e^{\beta_2Volum}}{e^{\beta_0}\times e^{\beta_1\times 0}\times e^{\beta_2Volum}}=e^{\beta_1}$$如上，性别变量的发生比率为$e^{\beta1}$，表示男性患癌的发生比为女性患癌的发生比的$e^{\beta1}$倍，如果是对离散型自变量的系数，就可以如此解释。     

如果是连续型自变量也可以用类似的方法解释参数的含义，假设肿瘤的体积为$Volum_0$，当肿瘤体积增加一个单位后，体积为$Volum_0+1$，则：$$\frac{odds_{Volum_0+1}}{odds_{Volum_0}}=\frac{e^{\beta_0}\times e^{\beta_1\times Gender}\times e^{\beta_2{Volum_0+1}}}{e^{\beta_0}\times e^{\beta_1\times Gender}\times e^{\beta_2Volum_0}}=e^{\beta_2}$$所以，在其他变量不变的情况下，肿瘤体积每增加一个单位，将会使患癌发生比变化$e^{\beta_2}$倍，这个倍数是相对于原来的$Volum_0$而言的。     

- 当$\beta_k$为正数时，发生比率将大于1，表示$x_k$每增加一个单位时，发生比会相应增加；
- 当$\beta_k$为负数时，发生比率将小于1，说明$x_k$每增加一个单位时，发生比会相应减小；
- 当$\beta_k$为0时，发生比率将等于1，表明无论$x_k$如何变化，都无法使发生比发生变化。   

### 分类模型的评估方法   

当构建完模型后，我们就可以利用模型对新样本进行预测，但预测效果的好坏该如何评估？常用的评估方法有混淆矩阵、ROC曲线、K-S曲线等。   

#### 混淆矩阵   

以肿瘤的预测为例，对于数据集存在两种分类：良性和恶性。构建 Logistic 回归模型进行预测后会得到样本所属的类别，这样我们可以得到两列数据：一是真实的分类序列，另一个是模型预测出来的分类序列。我们根据这两列数据可以得到一个汇总的列联表，该列联表就被称为**混淆矩阵**。如下：

![](.\charts\混淆矩阵.png)   

如上混淆矩阵，0表示良性（负例），1表示恶性（正例，一般被理解为研究者所感兴趣或关心的那个分类）。    

混淆矩阵中字母表示对应组合下的样本量，首先介绍一下混淆矩阵各部分代表的内容：
- A：表示正确预测的负例的样本个数，用TN表示。
- B：表示预测为负例但实际为正例的样本个数，用FN表示。
- C：表示预测为正例但实际为负例的样本个数，用FP表示。
- D：表示正确预测的正例的样本个数，用TP表示。
- A+B：表示预测负例的样本个数，用PN表示。
- C+D：表示预测正例的样本个数，用PP表示。
- A+C：表示实际负例的样本个数，用AN表示。
- B+D：表示实际正例的样本个数，用AP表示。

接下来我们介绍几个指标的定义：
- 准确率：表示正确预测的正负例样本数与所有样本数量的比值，即(A+D)/(A+B+C+D)，该指标用来衡量模型对整体数据的预测效果，用Accuracy表示。
- 正例覆盖率：表示正确覆盖的正例数在实际正例数中的比例，即D/(B+D)，该指标反映的是模型能够在多大程度上覆盖所关心的类别，用Sensitivity表示。
- 负例覆盖率：表示正确预测的负例在实际负例数中的比例，即A/(A+C)，用Specificity表示。
- 正例命中率：与正例覆盖率比较相似，表示正确预测的正例数在预测正例数中的比例，即D/(C+D)，这个指标在做市场营销的时候非常有用，例如对预测的目标人群做活动，实际响应的人越多，说名模型越能够刻画出关心的类别，用Precision表示。    

如果使用混淆矩阵评估模型的好坏，一般会选择准确率Accuracy指标、整理覆盖率Sensitivity指标和负例覆盖率Specificity指标。这三个指标越高，说明模型越理想。     

混淆矩阵的构造可以通过Pandas模块中的crosstab函数实现，也可以借助于sklearn子模块metrics中的confusion_matrix函数完成。      

#### ROC曲线    

ROC曲线是通过可视化的方式实现模型好坏的评估，它使用两个指标进行绘制，其中x轴为 1-Specificity（负例判错率），y轴为 Sensitivity（正例覆盖率）。    

在绘制ROC曲线时，通过考虑不同阈值下的 Sensitivity 与 1-Specificity 之间的组合变化。    

对于 Logistic 模型来讲，通常会选择 0.5 作为判别类型的阈值，若模型得分大于 0.5 则判断样本为正例，某则为负例。但是在进行模型评估时，通常会选择不同的阈值，计算对应的 Sensitivity 与 1-Specificity ，进而得到ROC曲线。     

通常我们希望正例覆盖率（Sensitivity）和负例覆盖率（Specificity）都比较高，模型的分类效果就会更好。ROC曲线的示意图如下：

![](.\charts\ROC曲线示意图.png)


如图，如果我们要求正例覆盖率（Sensitivity）和负例覆盖率（Specificity）都比较高的话，1-Specificity 就比较小，所绘制的ROC曲线下方所覆盖的面积（图中蓝色部分）就会比较大，这个面积称为AUC。    

在做模型评估时，我们希望AUC的值越大越好，通常情况下，当AUC在0.8以上时，模型就基本可以接受了。     

sklearn模块中提供了计算 Sensitivity 和 1-Specificity 的函数，函数名称为 roc_curve，该函数分布于子模块 metrics 中。     

#### K-S曲线

K-S曲线是另外一种评估模型的可视化方法，与ROC曲线的画法非常相似，具体如下：

1. 按照模型计算的Score（得分）值，从大到小排序
2. 取出10%、20%、...、90%所对应的分位数，并以此作为Score的阈值，计算 Sensitivity 和 1-Specificity 的值
3. 将10%、20%、...、90%分位点用作绘图的x轴，将 Sensitivity 和 1-Specificity 两个指标用作绘图的y轴，得到两条曲线

Python中没有直接提供绘制K-S曲线的函数，所以需要手动编写绘制K-S曲线的代码。

自定义绘制K-S曲线的函数后，运用事先准备的测试数据绘制对应的K-S曲线。
```
virtual_data = pd.read_excel('virtual_data.xlsx')
plot_ks(y_test = virtual_data.Class, 
        y_score = virtual_data.Score, 
        positive_flag = 'P')
```

![](.\charts\K-S曲线示意图.png)

如上图所示，两条曲线分别代表各分位点下的正例覆盖率和1-负例覆盖率，通过两条曲线很难评估模型的好坏，一般会使用最大的KS值作为衡量指标。    

KS值的计算公式为：KS = Sensitivity - (1 - Specificity) = Sensitivity + Specificity - 1。对于KS值而言，也是希望越大越好，通常情况下，当KS值大于0.4时，模型基本可以接受。    

### Logistic回归模型的应用  

应用部分所采用的数据集是手机设备收集的用户运动数据，来判断用户的运动状态，即：步行or跑步。     

该数据集一共包含88588条记录，6个与运动相关的自变量，其中3个与运动的加速度有关，另外三个与运动的方向有关。     

接下来将利用该数据集构建 Logistic 回归模型，并预测新样本所处的运动状态。     

#### 模型的构建    

首先要使用 Python 构建 Logistic 回归模型，可以借助于 sklearn 的子模块 linear_model，调用 LogisticRegression 类，有关该类的介绍如下：
```
LogisticRegression(penalty='12', dual=False, tol=0.0001, C=0.1, Fit_intercept=True, 
                    intercept_scaling=1, class_weight=None, randomm_state=None,
                    solver='liblinear', max_iter=100, multi_class='ovr', 
                    verbose=0, warm_start=False, n_jobs=1)
```
- penalty：为 Logistic 回归模型的目标函数添加正则化惩罚项，与线性回归模型类似，默认为L2正则。
- dual：bool类型参数，是否求解对偶形式，默认为False，只有当penalty参数为'L2'，solver参数为'liblinear'时，才可使用对偶形式。
- tol：用于指定模型跌倒收敛的阈值。
- C：用于指定惩罚项系数Lambda的倒数，值越小，正则化项越大。
- fit_intercept：bool类型参数，是否拟合模型的截距项，默认为True。
- intercept_scaling：当solver参数为'liblinear'时该参数有效，主要是为了降低X矩阵中人为设定的常数列1的影响。
- class_weight：用于指定因变量类别的权重，如果为字典，则通过字典的形式{class_label:weight}传递每个类别的权重；如果为字符串'balanced'，则每个分类的权重与实际样本中的比例成反比，当各分类权重存在严重不平衡时，设置为'balanced'会比较好；如果为None，则表示每个分类的权重相等。
- random_state：用于指定随机数生成器的种子。
- solver：用于指定求解目标函数最优化的算法，默认为'liblinear'，还有其他选项，如牛顿法'newton-cg'、L-BFGS拟牛顿法'lbfgs'。
- max_iter：指定模型求解过程中的最大迭代次数，默认为100.
- multi_class：如果因变量不止两个分类，可以通过该参数指定多分类问题的解决办法，默认采用'ovr'，即one-vs-rest方法，还可以指定'multinomial'，表示直接使用多分类逻辑回归模型（Softmax分类）。
- verbose：bool类型参数，是否输出模型迭代过程的信息，默认为0，表示不输出。
- warm_start：bool类型参数，是否基于上一次的训练结果继续训练模型，默认为False，表示每次迭代都是从头开始。
- n_jobs：指定模型运用时使用的CPU数量，默认为1，如果是-1，则表示使用所有可用的CPU。

这里做一点说明：当fit_intercept设置为True时，相当于在X数据集上认为的添加了常数列1，用于计算模型的截距项。     

LogisticRegression 类不仅可以针对二元问题做分类，还可以解决多元问题，通过设置参数multi_class为'multinomial'，实现Softmax分类，并利用随机梯度下降法求解参数。    

下面通过该类对手机设备数据进行建模。    

首先导入第三方模块并读取数据。
```
# 导入所需要的模块
import pandas as pd
import numpy as np
from sklearn import linear_model
from sklearn import model_selection
# 读取数据
sports = pd.read_csv("Run or Walk.csv")
```
查看前五行数据如下：
```
# 查看前五行数据
sports.head()
        date	time	username	activity	acceleration_x	acceleration_y	acceleration_z	gyro_x	gyro_y	gyro_z
0	2017/6/30	13:51:15:847724020	viktor	0	0.2650	-0.7814	-0.0076	-0.0590	0.0325	-2.9296
1	2017/6/30	13:51:16:246945023	viktor	0	0.6722	-1.1233	-0.2344	-0.1757	0.0208	0.1269
2	2017/6/30	13:51:16:446233987	viktor	0	0.4399	-1.4817	0.0722	-0.9105	0.1063	-2.4367
3	2017/6/30	13:51:16:646117985	viktor	0	0.3031	-0.8125	0.0888	0.1199	-0.4099	-2.9336
4	2017/6/30	13:51:16:846738994	viktor	0	0.4814	-0.9312	0.0359	0.0527	0.4379	2.4922
```
如上所示，前三列数据对我们来说没有用处，第4列数据为运动状态，也即我们所要预测的y值，而从第5列开始为运动过程中的运动数据，即用于建模的自变量X，下面先对数据做预处理，提取出我们想要的数据。
```
# 提取自变量名称
predictors = sports.columns[4:]
# 构建自变量矩阵
X = sports.loc[:,predictors]
# 提取因变量y
y = sports.activity
# 将数据集拆分为训练集和测集
X_train, X_test, y_train, t_test = model_selection.train_test_split(X, y, test_size = 0.25, random_state = 1234)
```
自此数据预处理完成，接下来我们利用 sklearn 的子模块 linear_model 中的函数 LogisticRegression 进行建模。
```
# 利用训练集进行建模
sklearn_logistic = linear_model.LogisticRegression()
sklearn_logistic.fit(X_train, y_train)
```
模型建立完成后，我们来查看返回的模型的各个参数
```
# 查看返回的模型的各个参数
print(sklearn_logistic.intercept_, sklearn_logistic.coef_)
[4.36637441] [[ 0.48695898  6.87517973 -2.44872468 -0.01385936 -0.16085022  0.13389695]]
```


### 小结
本篇介绍了无监督的聚类算法——Kmeans聚类，并详细讲述了理论知识，然后在两个数据集中进行了实践，特别是在对NBA球员数据集进行聚类之后，我们通过观察聚类的结果，可以对比球员的好坏，在实际应用中可以帮助我们进行人才的挑选。  

虽然该聚类方法强大而灵活，但却存在明显的缺点。首先，该算法对于异常点十分敏感（一种解决的方法是先删除异常点，以防止其对聚类结果的影响），因为中心点是通过样本均值确定的，该算法不适合发现非球星的簇，因为它是基于距离的方式来判断样本之间的相似度（如果以某一个样本点为参照，假设距离其不超过某个值就认为是同一个簇中的样本，很明显用图形表示为以该样本点为圆心，在距离为半径的范围内都认为与其在同一个簇中）。  

通过本篇对Kmeans的介绍及应用，我们可以应用到实际工作中来解决某些分类问题。

### 后记
*本篇博文是笔者学习刘顺祥老师所著书籍《从零开始学Python数据分析与挖掘》后整理的部分笔记，文中引用了大量的原书内容，修正了书中部分错误代码，并加入了部分笔者自己的理解。*    

笔者在整理过程中，对书中作者提及但没有详细解释的概念尽可能地进行了解释，以及加入自己的理解，学习一个模型、明白如何构建模型以及用于预测过程比较简单，但是要理解模型背后的数学意义及原理是十分困难的，笔者尽可能地进行了介绍，但由于笔者才疏学浅，无法完全理解各个参数背后的数学原理及意义，还请有兴趣的读者自行上网查找相关资料。  

 - [ ] 参考自《从零开始学数据分析与挖掘》 [中]刘顺祥 著 
 - [ ] *完整代码及实践所用数据集等资料放置于：[Github](https://github.com/Yangyi001/Data_analysis/tree/master/Kmeans%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90)*